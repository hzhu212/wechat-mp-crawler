import base64
import datetime
import html
import json
import os
import random
import re
import time
import urllib

from bs4 import BeautifulSoup
import requests


cur_dir = os.path.dirname(os.path.abspath(__file__))
config_file = os.path.join(cur_dir, 'config.json')

config = {}
with open(config_file, 'r', encoding='utf8') as f:
    config = json.load(f)

if not config['output_dir']:
    config['output_dir'] = os.path.join(cur_dir, 'output')
if not os.path.exists(config['output_dir']):
    os.makedirs(config['output_dir'])
record_file = os.path.join(config['output_dir'], 'record.txt')

# è®¾ç½®ä¸€ä¸ªè®°å½•ï¼Œç”¨äºæ–­ç‚¹ç»­æŠ“ã€‚å¦‚æœéœ€è¦é‡æŠ“ï¼Œåˆ é™¤è®°å½•å³å¯
records = set()
if os.path.exists(record_file):
    with open(record_file, 'r', encoding='utf8') as f:
        records = set(line.rstrip('\n') for line in f)

# åªå¤„ç†ä»¥ä¸‹æ¶ˆæ¯ç±»å‹ï¼š 49 ä¸ºæ™®é€šå›¾æ–‡ç±»å‹ï¼Œå…¶ä»–ç±»å‹è·³è¿‡
ALLOWED_MSG_TYPE = [49, ]

re_comment_id = re.compile(r'\n\s*var +comment_id *= *[^\n]*[\'\"](\d+)[\'\"]')
re_appmsgid = re.compile(r'\n\s*var +appmsgid *= *[^\n]*[\'\"](\d+)[\'\"]')
re_appmsg_token = re.compile(r'\n\s*var +appmsg_token *= *[^\n]*[\'\"](\.+)[\'\"]')
re_devicetype = re.compile(r'\n\s*var +devicetype *= *[^\n]*[\'\"](.+)[\'\"]')
re_clientversion = re.compile(r'\n\s*var +clientversion *= *[^\n]*[\'\"](\d+)[\'\"]')
re_uin = re.compile(r'\n\s*window\.uin *= *[^\n]*[\'\"]([\w=\%]+)[\'\"]')
re_key = re.compile(r'\n\s*window\.key *= *[^\n]*[\'\"]([\w=\%]+)[\'\"]')
re_wxtoken = re.compile(r'\n\s*window\.wxtoken *= *[^\n]*[\'\"]([\w=\%]+)[\'\"]')


# æ–‡ç« ç±»
class Article(object):
    def __init__(self, id, datetime, title, author, digest, cover_url, content_url, source_url, content=None, index=0):
        self.id = id
        self.datetime = datetime
        self.title = title
        self.author = author
        self.digest = digest
        self.cover_url = cover_url
        self.content_url = content_url
        self.source_url = source_url
        self.content = content
        # index ä¸º 0 è¡¨ç¤ºå¤´æ¡æ–‡ç« ï¼Œæ¬¡æ¡æ–‡ç« ç¼–å·ä¾æ¬¡é€’å¢
        self.index = index

    def __getitem__(self, key):
        return getattr(self, key)

    def __str__(self):
        return str(self.__dict__)

    def __repr__(self):
        return f'<Article {self.__dict__}>'


def parse_fiddler_export(input_dir):
    """è§£æ Fiddler å¯¼å‡ºçš„æ–‡ç« åˆ—è¡¨æ–‡ä»¶ï¼Œå¾—åˆ°æ–‡ç« åˆ—è¡¨ã€‚
    å†å²æ–‡ç« åˆ—è¡¨ä¸€èˆ¬ä»¥ json æ–‡ä»¶ä¿å­˜ï¼Œå¯ç›´æ¥è½½å…¥å¹¶è§£æã€‚
    ä½†æœ‰ä¸ªç‰¹ä¾‹ï¼šhome é¡µçš„åˆ—è¡¨ä¿å­˜åœ¨ html å†…çš„ä¸€æ®µè„šæœ¬ä¸­ï¼Œéœ€è¦å•ç‹¬æå–ã€‚
    """
    for filename in sorted(os.listdir(input_dir)):
        ext = os.path.splitext(filename)[-1].lower()
        if ext not in ('.html', '.htm', '.json'):
            continue

        file = os.path.join(input_dir, filename)
        with open(file, 'r', encoding='utf8') as f:
            content = f.read()

        # è§£æ home é¡µ html æ–‡ä»¶ï¼Œè·å¾—é¦–å±æ–‡ç« åˆ—è¡¨
        if ext in ('.html', '.htm'):
            pattern = r'\n\s*var +msgList *= *[\'\"](\{[^\n]+\})[\'\"] *; *\n'
            json_str = re.search(pattern, content).group(1)
            # åŸå§‹å­—ç¬¦ä¸²ä¸­åŒ…å« &nbsp; &quote; &amp; ç­‰è½¬ç§»å­—ç¬¦ï¼Œéœ€è¦è§£è½¬ä¹‰ã€‚
            json_str = html.unescape(json_str)
            obj = json.loads(json_str)
            msg_list = obj['list']

        # è§£æç¿»é¡µ json æ–‡ä»¶ï¼Œè·å¾—åç»­å†å²æ–‡ç« åˆ—è¡¨
        elif ext == '.json':
            obj = json.loads(content)
            msg_list = json.loads(obj['general_msg_list'])['list']

        for msg in msg_list:
            if msg['comm_msg_info']['type'] not in ALLOWED_MSG_TYPE:
                continue
            article = Article(
                msg['comm_msg_info']['id'],
                msg['comm_msg_info']['datetime'],
                msg['app_msg_ext_info']['title'],
                msg['app_msg_ext_info']['author'],
                msg['app_msg_ext_info']['digest'],
                msg['app_msg_ext_info']['cover'],
                msg['app_msg_ext_info']['content_url'],
                msg['app_msg_ext_info']['source_url'],
                )
            article.datetime = datetime.datetime.fromtimestamp(article.datetime)
            yield article

            # æœ‰äº›æ¶ˆæ¯åŒ…å«å¤šç¯‡æ–‡ç« ï¼Œæ­¤å¤„è§£ææ¬¡æ¡
            if msg['app_msg_ext_info']['is_multi']:
                for idx, sub_msg in enumerate(msg['app_msg_ext_info']['multi_app_msg_item_list']):
                    article = Article(
                        msg['comm_msg_info']['id'],
                        msg['comm_msg_info']['datetime'],
                        sub_msg['title'],
                        sub_msg['author'],
                        sub_msg['digest'],
                        sub_msg['cover'],
                        sub_msg['content_url'],
                        sub_msg['source_url'],
                        index=idx + 1,
                        )
                    article.datetime = datetime.datetime.fromtimestamp(article.datetime)
                    yield article


def article_pipe(article_iter):
    """å¯¹æ–‡ç« åˆ—è¡¨è¿›ä¸€æ­¥å¤„ç†ã€‚
    åŒ…æ‹¬å¤„ç†å…ƒä¿¡æ¯ï¼ˆä¸»è¦æ˜¯å¯¹ url è§£è½¬ä¹‰ï¼‰ï¼ŒæŒ‰ç…§æ—¶é—´å€’æ’åºç­‰ã€‚
    """
    article_list = []
    for article in article_iter:
        for attr in ('cover_url', 'content_url', 'source_url'):
            new_url = html.unescape(getattr(article, attr)).replace(r'\/', '/')
            setattr(article, attr, new_url)
        article_list.append(article)

    article_list.sort(key=lambda article: article.datetime, reverse=True)
    return article_list


def get_comments(article, base_params, session):
    """ä»æ–‡ç« çš„ content_url å’Œ content ä¸­è§£æç›¸å…³å‚æ•°ï¼Œæ„é€ è¯·æ±‚è¯„è®ºçš„ URLï¼Œæœ€ç»ˆè·å¾—è¯„è®ºæ•°æ®

    æ–‡ç« æ¥å£ç¤ºä¾‹ï¼šhttps://mp.weixin.qq.com/s?__biz=MzAwMzU1ODAwOQ==&mid=2650332778&idx=1&sn=4acfcb69e9b63eec3efc4a9d94cc6cad&chksm=8335217cb442a86a61c8c3321e3741b7d7d6e28f9357d0a27f4353bf7a7896cae584bbc14387&scene=27#wechat_redirect
    è¯„è®ºæ¥å£ç¤ºä¾‹ï¼šhttps://mp.weixin.qq.com/mp/appmsg_comment?action=getcomment&scene=0&__biz=MzAwMzU1ODAwOQ==&appmsgid=2650332982&idx=1&comment_id=1054388621538770944&offset=0&limit=100&uin=MjI5MDQwNTIzNg%253D%253D&key=90610e7a4a02526ca52f96868014fc10dc52f9f6c531f3bbf72541688a88014ffd7682ec315b7d6434b87938f8b87c741aa41d31f90f951648a409365cef63aba4ec76ad21e3d671d09df460e17e87a5&pass_ticket=nQePwVT9BEpY%25252FozZHsy33LGDMUCXfgbCMKZTPCnDslELz4XHZ2AEXZVpKpF0yEeH&wxtoken=777&devicetype=Windows%26nbsp%3B10&clientversion=62070152&__biz=MzAwMzU1ODAwOQ%3D%3D&appmsg_token=1033_3iubBoXR%252F6I0xBAcSf2BpR9hk5tm6g5v3GSvo2wyPqnN7rYBSNgdvpjxA02o_wy57E25IktRF_ugMLDP&x5=0&f=json
    """
    content_url_params = dict(urllib.parse.parse_qsl(urllib.parse.urlparse(article.content_url).query))

    m_comment_id = re.search(re_comment_id, article.content)
    m_uin = re.search(re_uin, article.content)
    m_key = re.search(re_key, article.content)
    m_wxtoken = re.search(re_wxtoken, article.content)
    m_devicetype = re.search(re_devicetype, article.content)
    m_clientversion = re.search(re_clientversion, article.content)
    m_appmsg_token = re.search(re_appmsg_token, article.content)

    comment_url = 'https://mp.weixin.qq.com/mp/appmsg_comment'
    params = {
        'action': 'getcomment',
        'scene': 0,
        '__biz': base_params['__biz'],
        'appmsgid': content_url_params['mid'],
        'idx': content_url_params['idx'],
        'comment_id': (m_comment_id and m_comment_id.group(1)) or '',
        'offset': 0,
        'limit': 100,
        'uin': base_params.get('uin') or (m_uin and m_uin.group(1)) or '',
        'key': base_params.get('key') or (m_key and m_key.group(1)) or '',
        'wxtoken': base_params.get('wxtokenkey') or (m_wxtoken and m_wxtoken.group(1)) or '',
        'pass_ticket': base_params['pass_ticket'],
        'devicetype': (m_devicetype and m_devicetype.group(1)) or '',
        'clientversion': (m_clientversion and m_clientversion.group(1)) or '',
        'appmsg_token': base_params.get('appmsg_token') or (m_appmsg_token and m_appmsg_token.group(1)) or '',
        'x5': 0,
        'f': 'json',
    }
    response = session.get(comment_url, params=params)
    print(f'è°ƒè¯•è¯„è®ºæ‹‰å–æ¥å£ï¼š{response.request.url}')
    obj = response.json()
    comments = [
        {
            'nick_name': comm['nick_name'],
            'logo_url': comm['logo_url'],
            'content': comm['content'],
            'create_time': comm['create_time'],
            'like_num': comm['like_num'],
            'reply': None if not comm['reply']['reply_list'] else comm['reply']['reply_list'][0],
        }
        for comm in obj.get('elected_comment', [])]
    return comments


def _create_comment_html(comments):
    """æ„å»ºå…·æœ‰å¾®ä¿¡æ ·å¼çš„è¯„è®ºåŒº html"""

    if not comments:
        return ''

    comments_area = BeautifulSoup('''
        <div><style>
            .comment_block {
                position: relative;
                margin-bottom: 25px;
                font-size: 0.9em;
            }
            .logo_block {
                position: absolute;
                left: 0;
                width: 40px;
                padding-right: 5px;
                box-sizing: border-box;
            }
            .logo_block img {
                width: 100%;
            }
            .comment_meta {
                position: relative;
                margin-left: 40px;
                color: #999;
                font-size: 0.9em;
                height: 1.2em;
                line-height: 1em;
            }
            .comment_meta span {
                display: inline-block;
                position: absolute;
            }
            .comment_content {
                margin-left: 40px;
                margin-bottom: 5px;
                clear: both;
                line-height: 1.5em;
            }
        </style></div>
        ''', features='lxml')

    for comm in comments:
        reply_div = ''
        if comm['reply']:
            reply_div = f'''
                <div class="comment_meta" style="border-left: solid 3px #1AAD19;">
                    <span style="left: 0; padding-left: 5px;">ä½œè€…</span>
                    <span style="right: 0">ğŸ‘ {comm['reply'].get('reply_like_num', 0)}</span>
                </div>
                <div class="comment_content">{comm['reply']['content']}</div>
            '''
        comm_div = BeautifulSoup(f'''
            <div class="comment_block">
                <div class="logo_block"><img src="{comm['logo_url']}"/></div>
                <div class="comment_meta">
                    <span style="left: 0">{comm['nick_name']}</span>
                    <span style="right: 0">ğŸ‘ {comm['like_num']}</span>
                </div>
                <div class="comment_content">{comm['content']}</div>
                {reply_div}
            </div>
            ''', features='lxml')
        comments_area.append(comm_div)

    return comments_area


def modify_content(article, comments, session):
    """åŸåœ°ä¿®å¤æ–‡ç« å†…å®¹ï¼šæ¸…ç†æ— å…³æ ‡ç­¾ã€åµŒå…¥å›¾ç‰‡ã€æ·»åŠ å‘å¸ƒæ—¶é—´ã€æ·»åŠ è¯„è®ºä¿¡æ¯ç­‰"""
    soup = BeautifulSoup(article.content, features='lxml')

    # æ¸…ç†ä¸å¿…è¦çš„æ ‡ç­¾
    for tag in soup.find_all('script'):
        tag.decompose()

    # å°†å›¾ç‰‡ä»¥ base64 ç¼–ç åµŒå…¥åˆ° html æ–‡ä»¶ä¸­
    for img in soup.find_all('img'):
        url = img.attrs.get('data-src') or img.attrs.get('src')
        if not url:
            continue
        if url.startswith('//'):
            url = 'http:' + url
        img_type = img.attrs.get('data-type') or 'png'

        response = session.get(url)
        b64 = base64.b64encode(response.content).decode('ascii')
        img['src'] = f'data:image/{img_type};base64,{b64}'

    # æ·»åŠ å‘å¸ƒæ—¶é—´
    tag = soup.find(id='publish_time')
    if tag:
        tag.string = article.datetime.strftime('%Y-%m-%d %H:%M:%S')

    # å°†è¯„è®ºå†…å®¹è¿½åŠ åˆ° html æ–‡æ¡£æœ«å°¾
    comments_area = _create_comment_html(comments)
    if comments_area:
        tag = soup.find(class_='rich_media_area_primary_inner')
        if not tag:
            tag = soup.body
        tag.append(comments_area)
    new_content = soup.prettify()
    article.content = new_content


def parse_raw_http_request(request_text):
    """è§£æä» Fiddler ä¸­å¯¼å‡ºçš„ HTTP è¯·æ±‚æ–‡æœ¬ï¼Œç”¨äºè·å–è¯·æ±‚å‚æ•°ã€cookie ç­‰å­—æ®µ"""
    from http.server import BaseHTTPRequestHandler
    from io import BytesIO

    class HTTPRequest(BaseHTTPRequestHandler):
        def __init__(self, request_text):
            self.rfile = BytesIO(request_text)
            self.raw_requestline = self.rfile.readline()
            self.error_code = self.error_message = None
            self.parse_request()

        def send_error(self, code, message):
            self.error_code = code
            self.error_message = message

    request = HTTPRequest(request_text)
    return request


def parse_raw_cookie(cookie_str):
    """å°†åŸå§‹ cookie å­—ç¬¦ä¸²è§£æä¸º dict"""
    from http.cookies import SimpleCookie

    cookie = SimpleCookie()
    cookie.load(cookie_str)
    cookie = {key: morsel.value for key, morsel in cookie.items()}
    return cookie


def valid_filename(name):
    """è½¬ä¹‰æŸäº›å­—ç¬¦ï¼Œä½¿å­—ç¬¦ä¸²æˆä¸ºåˆæ³•çš„æ–‡ä»¶åã€‚
    åœ¨Windowsä¸­ï¼Œ/ \ : * ? " ' < > | è¿™å‡ ä¸ªå­—ç¬¦ä¸èƒ½å­˜åœ¨äºæ–‡ä»¶å¤¹åæˆ–æ–‡ä»¶åä¸­ï¼Œæ›¿æ¢ä¸ºä¸‹åˆ’çº¿ã€‚
    """
    new_name = re.sub(r'[\/\\\:\*\?\"\'\<\>\|]', '_', name)
    return new_name


def main():
    with open(os.path.join(config['input_dir'], config['raw_request']), 'rb') as f:
        raw_request = f.read()
    request = parse_raw_http_request(raw_request)

    common_headers = {
        'User-Agent': request.headers['User-Agent'],
        'Accept': request.headers['Accept'],
        'Connection': request.headers['Connection'],
        'Accept-Language': request.headers['Accept-Language'],
    }

    # è§£æè¯·æ±‚å‚æ•°ä¸ Cookiesï¼Œç•™å¾…ç¨å€™ä½¿ç”¨
    params = dict(urllib.parse.parse_qsl(urllib.parse.urlparse(request.path).query))
    cookies = parse_raw_cookie(request.headers['cookie'])
    params.update(cookies)

    with requests.Session() as session:
        session.headers.update(common_headers)
        session.cookies.update(cookies)

        for article in article_pipe(parse_fiddler_export(config['input_dir'])):
            # è·³è¿‡å†…å®¹ä¸ºç©ºçš„æ–‡ç« 
            if not article.content_url:
                continue

            fingerprint = f'{article.datetime:%Y%m%d}-{article.title}'
            print('-' * 80)
            print(fingerprint)

            # å¿½ç•¥å·²ç»æŠ“å–è¿‡çš„æ–‡ç« 
            if fingerprint in records:
                print(f'æ–‡ç« å·²åœ¨æŠ“å–è®°å½•ä¸­ï¼Œå¿½ç•¥')
                continue

            # å¿½ç•¥æ¬¡æ¡ä¸­åŒ…å«åŸæ–‡é“¾æ¥çš„æ–‡ç« ï¼Œä¸€èˆ¬æ˜¯å¹¿å‘Š
            if article.index > 0:
                print(f'è¯¥æ–‡ç« ä¸ºæ¬¡æ¡æ–‡ç« ï¼Œç¼–å· {article.index}')
                if article.source_url.strip():
                    print('è¯¥æ–‡ç« å¾ˆå¯èƒ½ä¸ºæ¨å¹¿æ–‡ç« ï¼Œå¿½ç•¥')
                    continue

            print(article)
            article.content = session.get(article.content_url).text

            # with open('tmp.html', 'w', encoding='utf8') as f:
            #     f.write(article.content)

            comments = get_comments(article, params, session)
            print(f'æ–‡ç« åŒ…å« {len(comments)} æ¡è¯„è®ºã€‚ç¤ºä¾‹ï¼š{comments[:1]}')
            modify_content(article, comments, session)

            filename = f'{article.datetime:%Y%m%d}-{article.title}.html'
            filename = valid_filename(filename)
            with open(os.path.join(config['output_dir'], filename), 'w', encoding='utf8') as f:
                f.write(article.content)

            # æŠ“å–å®Œæˆï¼Œå°†æ–‡ç« åŠ å…¥è®°å½•
            records.add(fingerprint)
            with open(record_file, 'a', encoding='utf8') as f:
                f.write(fingerprint + '\n')

            time.sleep(random.random() * 5)


if __name__ == '__main__':
    main()
